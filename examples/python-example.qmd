---
title: "Exemple Python : Analyse et Machine Learning"
subtitle: "Classification de donn√©es avec Scikit-learn"
author: "Parcours IA"
date: last-modified
format:
  html:
    code-fold: show
    code-tools: true
    toc: true
jupyter: python3
execute:
  enabled: true
  cache: true
---

## üêç Introduction

Ce notebook d√©montre une analyse compl√®te de donn√©es et un mod√®le de machine learning en Python, incluant :

- Chargement et exploration de donn√©es
- Visualisation
- Pr√©paration des donn√©es
- Entra√Ænement de mod√®les
- √âvaluation des performances

## üìö Importation des Biblioth√®ques

```{python}
# Manipulation de donn√©es
import numpy as np
import pandas as pd

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    confusion_matrix,
    roc_auc_score,
    roc_curve
)

# Configuration de l'affichage
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)

print("‚úÖ Biblioth√®ques import√©es avec succ√®s")
```

## üìä Chargement des Donn√©es

Nous utilisons le c√©l√®bre dataset Iris pour cette d√©monstration.

```{python}
# Charger le dataset Iris
iris = load_iris()
df = pd.DataFrame(
    data=iris.data, 
    columns=iris.feature_names
)
df['species'] = iris.target
df['species_name'] = df['species'].map({
    0: 'setosa', 
    1: 'versicolor', 
    2: 'virginica'
})

print(f"üìà Dataset charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes")
print("\nPremi√®res lignes :")
df.head()
```

## üîç Exploration des Donn√©es

### Statistiques Descriptives

```{python}
print("üìä Statistiques descriptives :")
df.describe()
```

### Distribution des Classes

```{python}
print("\nüå∏ Distribution des esp√®ces :")
class_dist = df['species_name'].value_counts()
print(class_dist)

# Visualisation
plt.figure(figsize=(10, 5))
class_dist.plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
plt.title('Distribution des Esp√®ces d\'Iris', fontsize=14, fontweight='bold')
plt.xlabel('Esp√®ce')
plt.ylabel('Nombre d\'√©chantillons')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## üìà Visualisation des Donn√©es

### Pairplot des Features

```{python}
# Cr√©er un pairplot pour visualiser les relations
sns.pairplot(
    df, 
    hue='species_name',
    diag_kind='kde',
    markers=['o', 's', 'D'],
    plot_kws={'alpha': 0.6}
)
plt.suptitle('Relations entre les Features', y=1.02, fontsize=16)
plt.tight_layout()
plt.show()
```

### Matrice de Corr√©lation

```{python}
# Calculer la matrice de corr√©lation
correlation_matrix = df[iris.feature_names].corr()

# Visualiser avec une heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    correlation_matrix, 
    annot=True, 
    fmt='.2f',
    cmap='coolwarm',
    square=True,
    linewidths=1,
    cbar_kws={"shrink": 0.8}
)
plt.title('Matrice de Corr√©lation des Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nüí° Observations :")
print("- Petal length et petal width sont fortement corr√©l√©es (0.96)")
print("- Ces features semblent tr√®s discriminantes pour la classification")
```

## üîß Pr√©paration des Donn√©es

```{python}
# S√©parer features et target
X = df[iris.feature_names].values
y = df['species'].values

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.3, 
    random_state=42,
    stratify=y
)

# Normalisation des features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚úÖ Donn√©es pr√©par√©es :")
print(f"   - Training set : {X_train.shape[0]} √©chantillons")
print(f"   - Test set : {X_test.shape[0]} √©chantillons")
print(f"   - Features : {X_train.shape[1]}")
```

## ü§ñ Entra√Ænement des Mod√®les

### Mod√®le 1 : R√©gression Logistique

```{python}
# Cr√©er et entra√Æner le mod√®le
logreg = LogisticRegression(
    random_state=42,
    max_iter=200,
    multi_class='ovr'
)
logreg.fit(X_train_scaled, y_train)

# Pr√©dictions
y_pred_logreg = logreg.predict(X_test_scaled)
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)

print(f"üìä R√©gression Logistique")
print(f"   Accuracy : {accuracy_logreg:.2%}")
```

### Mod√®le 2 : Random Forest

```{python}
# Cr√©er et entra√Æner le mod√®le
rf = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    max_depth=5
)
rf.fit(X_train, y_train)

# Pr√©dictions
y_pred_rf = rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

print(f"üå≥ Random Forest")
print(f"   Accuracy : {accuracy_rf:.2%}")
```

## üìä √âvaluation des Performances

### Matrice de Confusion

```{python}
# Matrice de confusion pour Random Forest
cm = confusion_matrix(y_test, y_pred_rf)

# Visualisation
plt.figure(figsize=(10, 8))
sns.heatmap(
    cm, 
    annot=True, 
    fmt='d',
    cmap='Blues',
    xticklabels=iris.target_names,
    yticklabels=iris.target_names,
    square=True,
    linewidths=1
)
plt.title('Matrice de Confusion - Random Forest', fontsize=14, fontweight='bold')
plt.ylabel('Vraie classe')
plt.xlabel('Classe pr√©dite')
plt.tight_layout()
plt.show()
```

### Rapport de Classification

```{python}
print("üìã Rapport de Classification D√©taill√© (Random Forest) :\n")
print(classification_report(
    y_test, 
    y_pred_rf,
    target_names=iris.target_names,
    digits=3
))
```

### Importance des Features (Random Forest)

```{python}
# Extraire l'importance des features
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# Visualisation
plt.figure(figsize=(10, 6))
plt.barh(
    feature_importance['feature'], 
    feature_importance['importance'],
    color='#4ECDC4'
)
plt.xlabel('Importance')
plt.title('Importance des Features - Random Forest', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\nüîç Features les plus importantes :")
print(feature_importance.to_string(index=False))
```

## üìà Comparaison des Mod√®les

```{python}
# R√©sum√© des performances
results = pd.DataFrame({
    'Mod√®le': ['R√©gression Logistique', 'Random Forest'],
    'Accuracy': [accuracy_logreg, accuracy_rf],
    'Pr√©cision': [
        accuracy_logreg,
        accuracy_rf
    ]
})

print("\nüèÜ Comparaison des Mod√®les :")
print(results.to_string(index=False))

# Visualisation
plt.figure(figsize=(10, 6))
x = np.arange(len(results))
width = 0.35

plt.bar(x, results['Accuracy'], width, label='Accuracy', color='#FF6B6B', alpha=0.8)
plt.xlabel('Mod√®le')
plt.ylabel('Score')
plt.title('Comparaison des Performances', fontsize=14, fontweight='bold')
plt.xticks(x, results['Mod√®le'])
plt.ylim(0.85, 1.0)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
```

## üéØ Conclusions

```{python}
print("=" * 60)
print("üìå CONCLUSIONS DE L'ANALYSE")
print("=" * 60)
print(f"\n‚úÖ Meilleur mod√®le : Random Forest")
print(f"   Accuracy : {accuracy_rf:.2%}")
print(f"\nüí° Insights cl√©s :")
print(f"   - Les features 'petal' sont les plus discriminantes")
print(f"   - Excellente s√©parabilit√© des classes")
print(f"   - Mod√®le pr√™t pour la production")
print("\n" + "=" * 60)
```

---

::: {.callout-note}
## üìö Pour aller plus loin

- Testez d'autres algorithmes (SVM, Gradient Boosting)
- Optimisez les hyperparam√®tres avec GridSearchCV
- Ajoutez de la validation crois√©e
- Explorez le feature engineering avanc√©
:::
