---
title: "Delivery de Projet IA"
subtitle: "Livrer de la valeur de maniÃ¨re itÃ©rative et continue"
date: last-modified
format:
  html:
    toc: true
    toc-depth: 3
---

## ğŸš€ L'Art de Livrer en IA

La delivery en IA nÃ©cessite une approche spÃ©cifique qui combine :

- **ItÃ©rations rapides** pour valider les hypothÃ¨ses
- **ExpÃ©rimentation structurÃ©e** pour tester diffÃ©rentes approches
- **Feedback continu** pour ajuster la direction
- **DÃ©ploiement progressif** pour minimiser les risques

## ğŸ“… Cycle de Delivery IA

### Vue d'Ensemble

```
Discovery â†’ POC â†’ MVP â†’ V1 â†’ AmÃ©lioration Continue
  (2-4w)   (2-4w) (6-8w) (variable)  (ongoing)
```

### Phase 1 : Discovery (2-4 semaines)

**Objectif :** Explorer et valider la faisabilitÃ©

**ActivitÃ©s :**

- Analyse exploratoire des donnÃ©es (EDA)
- Recherche de littÃ©rature
- Benchmark d'algorithmes existants
- DÃ©finition de la baseline
- Identification des quick wins

**Livrables :**

- Rapport d'EDA
- Baseline metrics
- FaisabilitÃ© technique validÃ©e
- Recommandations d'approches

::: {.callout-note}
**Success Criteria :**
- DonnÃ©es accessibles et de qualitÃ© suffisante
- Au moins une approche prometteuse identifiÃ©e
- Baseline dÃ©finie comme point de comparaison
:::

### Phase 2 : Proof of Concept (2-4 semaines)

**Objectif :** DÃ©montrer la viabilitÃ© technique

**ActivitÃ©s :**

- DÃ©veloppement d'un prototype minimal
- Test de 2-3 algorithmes diffÃ©rents
- Validation sur donnÃ©es de test
- Estimation de l'effort pour industrialisation

**Livrables :**

- Prototype fonctionnel
- Comparaison d'algorithmes
- MÃ©triques de performance
- Plan de dÃ©ploiement

::: {.callout-note}
**Success Criteria :**
- Performance supÃ©rieure Ã  la baseline
- Validation technique confirmÃ©e
- Estimation coÃ»ts/bÃ©nÃ©fices rÃ©aliste
:::

### Phase 3 : MVP (6-8 semaines)

**Objectif :** Livrer une premiÃ¨re version utilisable

**Sprint 1-2 : DÃ©veloppement Core**

- Feature engineering robuste
- EntraÃ®nement modÃ¨le optimisÃ©
- Validation rigoureuse
- Tests unitaires

**Sprint 3-4 : Industrialisation**

- API REST pour infÃ©rences
- Pipeline de donnÃ©es automatisÃ©
- Monitoring de base
- Documentation

**Sprint 5-6 : IntÃ©gration & Tests**

- IntÃ©gration avec systÃ¨mes existants
- Tests d'intÃ©gration
- Tests de charge
- Ajustements finaux

**Livrables :**

- ModÃ¨le en production
- API documentÃ©e
- Dashboard de monitoring
- Guide utilisateur

### Phase 4 : AmÃ©lioration Continue

**Objectif :** Optimiser et maintenir la valeur

**ActivitÃ©s rÃ©currentes :**

- Monitoring des performances
- DÃ©tection de drift
- Retraining pÃ©riodique
- Ajout de features
- Optimisation continue

## ğŸ¯ Sprint Planning pour l'IA

### Structure de Sprint AdaptÃ©e

**Sprint de 2 semaines - Allocation typique :**

- **40%** : ExpÃ©rimentation et modÃ©lisation
- **30%** : Engineering et infrastructure
- **20%** : Tests et validation
- **10%** : Documentation et revue

### Planning Poker pour l'IA

Adapter l'estimation avec des critÃ¨res spÃ©cifiques :

**Facteurs de ComplexitÃ© :**

- DisponibilitÃ© des donnÃ©es (1-5)
- ComplexitÃ© de l'algorithme (1-5)
- QualitÃ© du code existant (1-5)
- DÃ©pendances externes (1-5)

**Ã‰chelle de Points :**

- **1 point** : TÃ¢che simple, 2-4h (ex: ajout d'une feature basique)
- **2 points** : TÃ¢che claire, 4-8h (ex: test d'un algorithme standard)
- **3 points** : ComplexitÃ© moyenne, 1 jour (ex: feature engineering)
- **5 points** : Complexe, 2-3 jours (ex: nouveau modÃ¨le)
- **8 points** : TrÃ¨s complexe, 3-5 jours (ex: architecture complÃ¨te)
- **13 points** : Trop gros, Ã  dÃ©couper

### Backlog StructurÃ©

**Types de User Stories :**

```markdown
# Story MÃ©tier
En tant que [utilisateur]
Je veux [fonctionnalitÃ©]
Afin de [bÃ©nÃ©fice]

CritÃ¨res d'acceptation:
- [ ] CritÃ¨re 1
- [ ] CritÃ¨re 2

# Story Technique
En tant que data scientist
Je veux [capacitÃ© technique]
Afin de [amÃ©liorer le modÃ¨le/systÃ¨me]

# Spike d'Exploration
Investiguer [sujet]
Time-box: [durÃ©e]
Objectif: [apprentissage attendu]
```

## ğŸ”¬ Gestion des ExpÃ©rimentations

### Workflow d'ExpÃ©rimentation

```
HypothÃ¨se â†’ ExpÃ©rimentation â†’ Ã‰valuation â†’ DÃ©cision
    â†‘                                         â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Apprentissage â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tracking avec MLflow

```python
import mlflow
import mlflow.sklearn

# Configuration de l'expÃ©rimentation
mlflow.set_experiment("churn_prediction")

with mlflow.start_run(run_name="random_forest_v1"):
    # Logger les paramÃ¨tres
    params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 5
    }
    mlflow.log_params(params)
    
    # EntraÃ®ner le modÃ¨le
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    
    # Logger les mÃ©triques
    metrics = {
        "accuracy": model.score(X_test, y_test),
        "f1_score": f1_score(y_test, model.predict(X_test)),
        "auc_roc": roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    }
    mlflow.log_metrics(metrics)
    
    # Logger le modÃ¨le
    mlflow.sklearn.log_model(model, "model")
    
    # Logger des artifacts
    mlflow.log_artifact("feature_importance.png")
```

### Registre d'ExpÃ©rimentations

Maintenir un registre pour chaque expÃ©rimentation :

| Run ID | Date | Algorithme | Params | Accuracy | F1 | Notes | Status |
|--------|------|------------|--------|----------|----|----|--------|
| exp-001 | 01/12 | RandomForest | n_est=100 | 0.82 | 0.79 | Baseline | ArchivÃ© |
| exp-002 | 05/12 | XGBoost | lr=0.1 | 0.85 | 0.83 | Meilleur | Produit |

## ğŸ§ª Tests et Validation

### Pyramide de Tests ML

```
        Tests A/B (Production)
              â†‘
         Tests d'IntÃ©gration
              â†‘
        Tests de ModÃ¨le
              â†‘
         Tests Unitaires
```

### Tests Unitaires (Data & Code)

```python
import pytest
import pandas as pd
from src.preprocessing import clean_data

def test_clean_data_removes_nulls():
    # Arrange
    df = pd.DataFrame({
        'feature1': [1, 2, None, 4],
        'feature2': [5, None, 7, 8]
    })
    
    # Act
    result = clean_data(df)
    
    # Assert
    assert result.isnull().sum().sum() == 0
    
def test_clean_data_maintains_shape():
    df = pd.DataFrame({'col1': [1, 2, 3]})
    result = clean_data(df)
    assert len(result.columns) >= len(df.columns)
```

### Tests de ModÃ¨le

```python
def test_model_performance_above_threshold():
    """Le modÃ¨le doit avoir une accuracy > 0.80"""
    accuracy = model.score(X_test, y_test)
    assert accuracy > 0.80, f"Accuracy {accuracy} below threshold"

def test_model_predictions_in_valid_range():
    """Les prÃ©dictions doivent Ãªtre dans [0, 1]"""
    predictions = model.predict_proba(X_test)
    assert (predictions >= 0).all() and (predictions <= 1).all()

def test_model_inference_latency():
    """L'infÃ©rence doit prendre moins de 100ms"""
    import time
    start = time.time()
    _ = model.predict(X_test[:1000])
    latency = (time.time() - start) / 1000
    assert latency < 0.1, f"Latency {latency}s above threshold"
```

### Tests de Non-RÃ©gression

```python
def test_model_performance_vs_baseline():
    """Le nouveau modÃ¨le doit Ãªtre meilleur que la baseline"""
    baseline_accuracy = 0.75  # StockÃ© depuis la version prÃ©cÃ©dente
    new_accuracy = new_model.score(X_test, y_test)
    assert new_accuracy >= baseline_accuracy - 0.02  # TolÃ©rance de 2%
```

## ğŸš¢ DÃ©ploiement

### StratÃ©gies de DÃ©ploiement

**Blue-Green Deployment**

```
Production (Blue)  â†â”€â”€â”€ Traffic 100%
    â†“ [Deploy new version]
Staging (Green)    â†â”€â”€â”€ Traffic 0%
    â†“ [Validation]
Production (Green) â†â”€â”€â”€ Traffic 100%
Production (Blue)  â†â”€â”€â”€ Archived
```

**Canary Release**

```
Version 1.0 â†â”€â”€â”€ Traffic 95%
Version 2.0 â†â”€â”€â”€ Traffic 5%
    â†“ [Monitor metrics]
Version 2.0 â†â”€â”€â”€ Traffic 10% â†’ 25% â†’ 50% â†’ 100%
```

**A/B Testing**

```
Control (A)    â†â”€â”€â”€ Random 50% users
Treatment (B)  â†â”€â”€â”€ Random 50% users
    â†“ [Compare metrics]
Winner deployed to 100%
```

### Pipeline CI/CD

```yaml
# .github/workflows/ml-deploy.yml
name: ML Model Deployment

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Tests
        run: |
          pytest tests/ --cov
      
  train:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Train Model
        run: |
          python train.py
      - name: Validate Performance
        run: |
          python validate.py
      - name: Upload Model
        uses: actions/upload-artifact@v3
        with:
          name: model
          path: model.pkl
  
  deploy-staging:
    needs: train
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to Staging
        run: |
          # Deploy logic
      - name: Run Integration Tests
        run: |
          pytest tests/integration/
  
  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to Production
        run: |
          # Deploy with canary strategy
```

## ğŸ“Š Monitoring en Production

### MÃ©triques Ã  Surveiller

**Performance du ModÃ¨le**

- Accuracy, Precision, Recall en temps rÃ©el
- Distribution des prÃ©dictions
- Temps d'infÃ©rence (p50, p95, p99)

**QualitÃ© des DonnÃ©es**

- Data drift (distribution features)
- Concept drift (relation feature-target)
- Valeurs manquantes
- Outliers dÃ©tectÃ©s

**Infrastructure**

- CPU/Memory utilisation
- Latence API
- Taux d'erreur
- Throughput (req/s)

### Alertes Automatiques

```python
# Configuration d'alertes
alerts = {
    "accuracy_drop": {
        "metric": "accuracy",
        "threshold": 0.75,
        "window": "1h",
        "action": "notify_team"
    },
    "high_latency": {
        "metric": "p95_latency",
        "threshold": 500,  # ms
        "window": "5m",
        "action": "scale_up"
    },
    "data_drift": {
        "metric": "drift_score",
        "threshold": 0.7,
        "window": "1d",
        "action": "trigger_retrain"
    }
}
```

### Dashboard de Monitoring

Ã‰lÃ©ments essentiels d'un dashboard :

1. **Vue d'ensemble** : SantÃ© globale du systÃ¨me
2. **Performance modÃ¨le** : MÃ©triques clÃ©s en temps rÃ©el
3. **QualitÃ© donnÃ©es** : Drift detection, anomalies
4. **Infrastructure** : Ressources, latence
5. **Business metrics** : Impact mÃ©tier rÃ©el

## ğŸ”„ Maintenance et Retraining

### StratÃ©gie de Retraining

**Retraining PÃ©riodique**

- FrÃ©quence fixe (ex: mensuelle)
- IndÃ©pendant des performances
- AdaptÃ© Ã  donnÃ©es stables

**Retraining Conditionnel**

- DÃ©clenchÃ© par drift detection
- BasÃ© sur dÃ©gradation performance
- Plus rÃ©actif et efficace

**Retraining Continu**

- En ligne (online learning)
- Adaptatif en temps rÃ©el
- Complexe Ã  mettre en Å“uvre

### Checklist de Retraining

Avant de dÃ©ployer un modÃ¨le retrained :

- [ ] Nouvelles donnÃ©es validÃ©es (qualitÃ©)
- [ ] Retraining sur donnÃ©es reprÃ©sentatives
- [ ] Performance validÃ©e sur test set
- [ ] Pas de rÃ©gression vs. version prÃ©cÃ©dente
- [ ] Tests de non-rÃ©gression passÃ©s
- [ ] Audit de biais rÃ©alisÃ©
- [ ] Documentation mise Ã  jour
- [ ] Approbation obtenue

## ğŸ“ˆ Mesure de la Valeur LivrÃ©e

### MÃ©triques de Delivery

**VÃ©locitÃ©**

- Story points livrÃ©s par sprint
- Tendance sur 6 sprints

**Lead Time**

- Temps idÃ©e â†’ production
- Objectif : < 3 mois pour MVP

**Cycle Time**

- Temps dÃ©veloppement â†’ dÃ©ploiement
- Objectif : < 2 semaines

**Deployment Frequency**

- Nombre de dÃ©ploiements / mois
- Objectif : â‰¥ 2 dÃ©ploiements / mois

### ROI et Impact MÃ©tier

Mesurer l'impact rÃ©el :

- **Gains de productivitÃ©** : Temps Ã©conomisÃ©
- **RÃ©duction de coÃ»ts** : Ã‰conomies rÃ©alisÃ©es
- **Augmentation de revenus** : Upsell, rÃ©tention
- **AmÃ©lioration qualitÃ©** : RÃ©duction erreurs

**Calcul de ROI :**

```
ROI = (Gains - CoÃ»ts) / CoÃ»ts Ã— 100%

Exemple:
Gains annuels: 500kâ‚¬
CoÃ»ts projet: 200kâ‚¬
ROI = (500k - 200k) / 200k = 150%
```

---

::: {.callout-note}
## ğŸ¯ FÃ©licitations !
Vous maÃ®trisez maintenant les trois piliers de la gestion de projet IA : [Cadrage](cadrage.qmd), [Gouvernance](gouvernance.qmd), et **Delivery** !
:::
