---
title: "Niveau Interm√©diaire"
subtitle: "Approfondissement en Machine Learning et Deep Learning"
date: last-modified
format:
  html:
    toc: true
    toc-depth: 3
---

## üöÄ Bienvenue au Niveau Interm√©diaire

Vous ma√Ætrisez les bases ? Il est temps d'approfondir vos connaissances avec des algorithmes avanc√©s, le deep learning, et les meilleures pratiques de la data science moderne.

## üìñ Module 1 : Algorithmes Avanc√©s

### Arbres de D√©cision et For√™ts Al√©atoires

Les arbres de d√©cision sont des mod√®les puissants et interpr√©tables.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

# Arbre de d√©cision simple
tree = DecisionTreeClassifier(max_depth=5, random_state=42)
tree.fit(X_train, y_train)

# For√™t al√©atoire (ensemble)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
print(f"Score RF: {rf.score(X_test, y_test):.2%}")
```

::: {.callout-note}
#### Avantages des Random Forests
- R√©duction du surapprentissage
- Gestion des variables cat√©gorielles
- Importance des features
- Performance robuste
:::

### Gradient Boosting

Technique d'ensemble bas√©e sur le boosting s√©quentiel.

```python
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb

# Gradient Boosting classique
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)
gb.fit(X_train, y_train)

# XGBoost (plus rapide et performant)
xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)
xgb_model.fit(X_train, y_train)

# LightGBM (encore plus efficace)
lgb_model = lgb.LGBMClassifier(n_estimators=100)
lgb_model.fit(X_train, y_train)
```

### Support Vector Machines (SVM)

Algorithmes puissants pour la classification et la r√©gression.

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Important : normaliser les donn√©es pour SVM
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVM avec kernel RBF
svm = SVC(kernel='rbf', C=1.0, gamma='scale')
svm.fit(X_train_scaled, y_train)
```

## üß† Module 2 : Introduction au Deep Learning

### R√©seaux de Neurones Artificiels

Architecture de base du deep learning.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Cr√©ation d'un r√©seau simple
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(input_dim,)),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation='softmax')
])

# Compilation
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Entra√Ænement
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)
```

### R√©seaux de Neurones Convolutifs (CNN)

Sp√©cialis√©s pour le traitement d'images.

```python
# CNN pour la classification d'images
cnn_model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

cnn_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

### R√©seaux R√©currents (RNN/LSTM)

Pour les s√©quences temporelles et le NLP.

```python
from tensorflow.keras.layers import LSTM, GRU

# Mod√®le LSTM pour s√©ries temporelles
lstm_model = keras.Sequential([
    layers.LSTM(50, return_sequences=True, input_shape=(timesteps, features)),
    layers.LSTM(50),
    layers.Dense(1)
])

lstm_model.compile(optimizer='adam', loss='mse')
```

## üîß Module 3 : Feature Engineering Avanc√©

### Transformation de Variables

```python
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, RobustScaler,
    PolynomialFeatures, PowerTransformer
)

# Normalisation robuste aux outliers
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# Cr√©ation de features polynomiales
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Transformation de Box-Cox
pt = PowerTransformer(method='box-cox')
X_transformed = pt.fit_transform(X_positive)
```

### S√©lection de Features

```python
from sklearn.feature_selection import (
    SelectKBest, f_classif,
    RFE, SelectFromModel
)

# S√©lection univari√©e
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)

# Recursive Feature Elimination
rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)

# S√©lection bas√©e sur l'importance
selector = SelectFromModel(RandomForestClassifier(), threshold='median')
X_important = selector.fit_transform(X, y)
```

### Encodage de Variables Cat√©gorielles

```python
from sklearn.preprocessing import (
    LabelEncoder, OneHotEncoder,
    OrdinalEncoder, TargetEncoder
)

# One-Hot Encoding
ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
X_encoded = ohe.fit_transform(X_categorical)

# Target Encoding (utile pour haute cardinalit√©)
from category_encoders import TargetEncoder
te = TargetEncoder()
X_target_encoded = te.fit_transform(X_categorical, y)
```

## üìä Module 4 : Optimisation et Tuning

### Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from scipy.stats import randint, uniform

# Grid Search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)
print(f"Meilleurs param√®tres: {grid_search.best_params_}")

# Random Search (plus efficace)
param_distributions = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(5, 50),
    'min_samples_split': randint(2, 20)
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(),
    param_distributions,
    n_iter=50,
    cv=5,
    random_state=42
)
```

### Validation Avanc√©e

```python
from sklearn.model_selection import (
    StratifiedKFold, TimeSeriesSplit,
    cross_validate
)

# Validation stratifi√©e (classes d√©s√©quilibr√©es)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)

# Pour s√©ries temporelles
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
```

## üéØ Module 5 : Gestion du D√©s√©quilibre

### Techniques de R√©√©chantillonnage

```python
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

# SMOTE : Synthetic Minority Over-sampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Combinaison over-sampling et under-sampling
smotetomek = SMOTETomek(random_state=42)
X_balanced, y_balanced = smotetomek.fit_resample(X_train, y_train)
```

### Ajustement des Poids

```python
from sklearn.utils.class_weight import compute_class_weight

# Calculer les poids automatiquement
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))

# Utiliser dans le mod√®le
rf = RandomForestClassifier(class_weight='balanced')
rf.fit(X_train, y_train)
```

## üí° Module 6 : Interpr√©tabilit√©

### Importance des Features

```python
import matplotlib.pyplot as plt

# Random Forest feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.title("Importance des Features")
plt.show()
```

### SHAP Values

```python
import shap

# Cr√©er l'explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualisations
shap.summary_plot(shap_values, X_test)
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
```

## üöÄ Projets Pratiques

### Projet 1 : Pr√©diction de Churn

Pr√©dire le d√©part de clients avec des donn√©es d√©s√©quilibr√©es.

**Comp√©tences** : Gradient Boosting, SMOTE, feature engineering, optimisation

### Projet 2 : Classification d'Images Avanc√©e

Classifier des images complexes (CIFAR-10, ImageNet subset).

**Comp√©tences** : CNN, data augmentation, transfer learning

### Projet 3 : Pr√©diction de S√©ries Temporelles

Pr√©voir des ventes ou des prix avec des donn√©es temporelles.

**Comp√©tences** : LSTM, feature engineering temporel, validation temporelle

## üìö Ressources pour Approfondir

### Frameworks et Biblioth√®ques

- **Scikit-learn** : ML classique
- **TensorFlow/Keras** : Deep learning
- **PyTorch** : Deep learning recherche
- **XGBoost/LightGBM** : Gradient boosting

### Livres Avanc√©s

- "Deep Learning" - Ian Goodfellow
- "Pattern Recognition and Machine Learning" - Christopher Bishop
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman

## ‚úÖ Check-list de Comp√©tences

Avant de passer au niveau exp√©riment√© :

- [ ] Ma√Ætrise de Random Forests et Gradient Boosting
- [ ] Cr√©ation et entra√Ænement de r√©seaux de neurones
- [ ] Compr√©hension des CNN pour les images
- [ ] Utilisation de LSTM pour les s√©quences
- [ ] Feature engineering avanc√©
- [ ] Hyperparameter tuning efficace
- [ ] Gestion des donn√©es d√©s√©quilibr√©es
- [ ] Interpr√©tation de mod√®les complexes

---

::: {.callout-note}
## üéì Prochaine √âtape
Pr√™t pour les d√©fis de pointe ? D√©couvrez le [Niveau Exp√©riment√©](experimente.qmd) !
:::
